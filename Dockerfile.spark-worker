FROM adoptopenjdk:11-jre-hotspot

USER root

# Install Python and required packages
RUN apt-get update && \
    apt-get install -y \
    python3-pip \
    python3-dev \
    gcc \
    libc-dev \
    libpq-dev \
    wget \
    && rm -rf /var/lib/apt/lists/* \
    && pip3 install --no-cache-dir \
    pandas \
    psycopg2-binary \
    pyspark==3.1.2

# Install Spark
RUN wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz \
    && tar xvf spark-3.1.2-bin-hadoop3.2.tgz \
    && mv spark-3.1.2-bin-hadoop3.2 /opt/spark \
    && rm spark-3.1.2-bin-hadoop3.2.tgz

# Install Spark Excel dependency
RUN wget https://repo1.maven.org/maven2/com/crealytics/spark-excel_2.12/3.3.1_0.18.7/spark-excel_2.12-3.3.1_0.18.7.jar -P /opt/spark/jars/

# Set environment variables
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH

# Configure Spark worker
ENV SPARK_WORKER_WEBUI_PORT=8081
ENV SPARK_WORKER_LOG=/spark/logs
ENV SPARK_MASTER=spark://spark-master:7077

EXPOSE 8081

# Use a non-root user
RUN groupadd -r spark && useradd -r -g spark spark
RUN mkdir -p /spark/logs /opt/spark/work && chown -R spark:spark /spark /opt/spark/work
USER spark

CMD ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]